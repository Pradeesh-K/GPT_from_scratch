{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed25172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "# a sequence of tokens, how they are sequenced, we give start of sequence the model continues\n",
    "# Generatively Pre-trained Transformer\n",
    "# Character level language model\n",
    "# Tiny shakesphere - concatenated work of all shakesphere in text filter\n",
    "# We model how charcters follow if given context\n",
    "# Output is a shakesphere like text\n",
    "# NanoGPT . train GPT with any text 124 million params - 38 hours training time in a single 8xA100 40GB Node\n",
    "#  8xA100 40GB - One machine with 8 NVIDIA A100 GPUs, each with 40 GB of VRAM, connected via NVLink or PCIe for multi-GPU training\n",
    "# using OpenWebText data \n",
    "# OpenwebText has 38GB of text data (40GB using SI units) from 8,013,769 documents\n",
    "# train.py is 300 line training loop\n",
    "# model.py a 300 line GPT model definition\n",
    "# A multilayer perceptron is a fully connected feedforward net with at least three layers (input, hidden and output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775d5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32c860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42bf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7274abfd2c90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4# higher rate to converge faster for such a small model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embed = 384 # every head is 64 dimensions \n",
    "n_head = 6\n",
    "n_layer = 6 # on CPU reduce number of layers and embedding dimensions. karpathy needed 15 mins in A100 GPU in 5000 iters 1.48 is his loss\n",
    "dropout = 0.2 # randomly drop 20 % of the neurons\n",
    "torch.manual_seed(1337)\n",
    "# scaling up 1.4873 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6de77f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394, type of dataset: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f\"length of dataset in characters:  {len(text)}, type of dataset: {type(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e79616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "set_of_text = set(text) # {'e', 'X', 'y', 'N', 'J', '3', 'o', 'w', 'b', 'U', '\n",
    "list_of_text = list(set_of_text) # ['e', 'X', 'y', 'N', 'J', '3', 'o', 'w', 'b', 'U', \n",
    "print(chars) # ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size) # entire set of characters - all possible characters the model can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0187eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# tokenize - convert raw text to sequence of numbers based on vocabulary. this is char level tokenization. all chars\n",
    "# openi - bite pair encoding . 50000 tokens- hii there - integers between 0 to 50000. sub word encoding. short seq int with large vocab or large seq integer with small vocab\n",
    "# create a mapping from characters to integers\n",
    "# enumerate(iterable, start=0)\n",
    "# Using the enumerate() function is useful from the standpoint of its memory and computing efficiency compared to using for-loops since it returns the index and the corresponding item at one go.\n",
    "# this function assigns a count incrementing by 1 to each item of an iterable and helps us track iterations while looping through that object\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } #  enumerate(chars). Lookup table Each char gets a number {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4\n",
    "itos = { i:ch for i,ch in enumerate(chars) } #. Reverse lookup table of itos {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\" .. Each number gets a character\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers. For each char, outputs the corresponding number\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string. For each number outputs the correponding character and then combined as a string\n",
    "print(stoi)\n",
    "print(itos)\n",
    "print(encode(\"hii there\")) # list of integers that represent string\n",
    "print(decode(encode(\"hii there\"))) # reverse mapping, decode integers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5e14a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # text is that massive string, now it outputs a tensor a 1-1 encoding of the characters in text to numbers\n",
    "print(data.shape, data.dtype) # shape is 1 x 1115394 # torch.Size([1115394]) torch.int64\n",
    "print(data[:1000])\n",
    "# train validation split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data=data[n:]\n",
    "# we dont feed all at once, we sample random chunks and train on chunks at once\n",
    "# max chunk is blocksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ee31158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we dont feed all at once, we sample random chunks and train on chunks at once\n",
    "# # max chunk is blocksize \n",
    "# block_size = 8\n",
    "# train_data[:block_size+1] # tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]). context of 18, 47 56 comes next, 18,47,56 context 57 comes next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a88c3174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # 8 characters\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # for 0 to t characters, target is the corresponding t\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")\n",
    "\n",
    "# we could predict from 1 to block size, beyond this we have to truncate\n",
    "# mini batches multiple chunks stacked on top of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e16ba6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54],\n",
      "        [57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
      "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
      "        [43, 57, 58, 63,  6,  1, 58, 46],\n",
      "        [43,  1, 51, 39, 63,  1, 40, 43],\n",
      "        [58, 46, 43,  1, 43, 39, 56, 57],\n",
      "        [39, 58, 47, 53, 52, 12,  1, 37],\n",
      "        [53, 56, 43,  1, 21,  1, 41, 39],\n",
      "        [50, 39, 52, 63,  1, 47, 58, 57],\n",
      "        [56, 53, 63,  1, 42, 47, 42,  1],\n",
      "        [39, 51,  1, 39, 44, 56, 39, 47],\n",
      "        [17, 24, 21, 38, 13, 14, 17, 32],\n",
      "        [ 1, 39, 52, 42,  1, 45, 43, 50],\n",
      "        [ 1, 58, 46, 39, 58,  1, 42, 53],\n",
      "        [ 1, 61, 53, 59, 50, 42,  1, 21],\n",
      "        [59, 57, 40, 39, 52, 42,  1, 40],\n",
      "        [52, 42,  8,  0,  0, 23, 21, 26],\n",
      "        [45, 53, 42, 57,  0, 23, 43, 43],\n",
      "        [52,  1, 61, 39, 57,  1, 51, 53],\n",
      "        [39, 49, 12,  1, 27,  1, 58, 56],\n",
      "        [53, 44,  1, 57, 54, 43, 43, 41],\n",
      "        [57, 53, 52, 57,  8,  0,  0, 25],\n",
      "        [ 1, 42, 43, 44, 43, 41, 58,  1],\n",
      "        [21,  1, 61, 39, 52, 42, 43, 56],\n",
      "        [43, 43, 51,  5, 42,  1, 40, 59],\n",
      "        [45, 50, 63,  1, 52, 53, 61, 12],\n",
      "        [52, 53, 58,  8,  0, 25, 63,  1],\n",
      "        [53, 58,  6,  1, 51, 63,  1, 50]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([32, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39],\n",
      "        [43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47],\n",
      "        [ 1, 51, 39, 63,  1, 40, 43,  1],\n",
      "        [46, 43,  1, 43, 39, 56, 57, 10],\n",
      "        [58, 47, 53, 52, 12,  1, 37, 53],\n",
      "        [56, 43,  1, 21,  1, 41, 39, 51],\n",
      "        [39, 52, 63,  1, 47, 58, 57, 43],\n",
      "        [53, 63,  1, 42, 47, 42,  1, 57],\n",
      "        [51,  1, 39, 44, 56, 39, 47, 42],\n",
      "        [24, 21, 38, 13, 14, 17, 32, 20],\n",
      "        [39, 52, 42,  1, 45, 43, 50, 42],\n",
      "        [58, 46, 39, 58,  1, 42, 53,  1],\n",
      "        [61, 53, 59, 50, 42,  1, 21,  1],\n",
      "        [57, 40, 39, 52, 42,  1, 40, 47],\n",
      "        [42,  8,  0,  0, 23, 21, 26, 19],\n",
      "        [53, 42, 57,  0, 23, 43, 43, 54],\n",
      "        [ 1, 61, 39, 57,  1, 51, 53, 56],\n",
      "        [49, 12,  1, 27,  1, 58, 56, 39],\n",
      "        [44,  1, 57, 54, 43, 43, 41, 46],\n",
      "        [53, 52, 57,  8,  0,  0, 25, 17],\n",
      "        [42, 43, 44, 43, 41, 58,  1, 53],\n",
      "        [ 1, 61, 39, 52, 42, 43, 56,  6],\n",
      "        [43, 51,  5, 42,  1, 40, 59, 56],\n",
      "        [50, 63,  1, 52, 53, 61, 12,  0],\n",
      "        [53, 58,  8,  0, 25, 63,  1, 61],\n",
      "        [58,  6,  1, 51, 63,  1, 50, 53]], device='cuda:0')\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "# torch.manual_seed(1337) # seed is set so it is reproducable\n",
    "# batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "# block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate batch size number of random offsets# gets 4 random indices between 0 to len(data) - block_size as we need mini batches too\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # gets 8 chars from indices. stack as rows  4x8 tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # gets 8 chars  from index+1\n",
    "    x, y = x.to(device), y.to(device) # moved to device on creation \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "# for b in range(batch_size): # batch dimension # for each mini batch \n",
    "#     for t in range(block_size): # time dimension\n",
    "#         context = xb[b, :t+1] # does the context step 0 - 0: 0,1- 1:0,1,2-2\n",
    "#         target = yb[b,t]\n",
    "#         print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "# #\n",
    "# target = yb[b, t] → the token the model should predict at this position\n",
    "\n",
    "# Because yb is shifted by 1 from xb, the target is always the next token after the current context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every 500 iterations (eval_interval = 500), stop training and run estimate_loss()\n",
    "@torch.no_grad()  # loss calculation dont need to influence gradients # don't track gradients (saves memory and compute)\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()  # turn off dropout, layernorm stats freezing, etc.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) \n",
    "        for k in range(eval_iters):   # Run a forward pass through the model 200 times (each with a new batch from get_batch(split))\n",
    "            X, Y = get_batch(split) # get a mini-batch from train or val\n",
    "            logits, loss = model(X, Y) # forward pass\n",
    "            losses[k] = loss.item() # record scalar loss\n",
    "        out[split] = losses.mean() # average across eval_iters\n",
    "    model.train() # switch back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75602c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # tril variable\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei) # randomly prevent some nodes from communicating - training in ensemble of subnetworks this prevents overfit\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' multiple heads of attention in parallel'''\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1) # run the heads independently and concatenate them  \n",
    "        out = self.dropout(self.proj(out)) # projection is the outcome of the linear transformation of the result of concatenation\n",
    "        # # Projection allows the model to combine the information from all heads in a learnable way, instead of keeping them siloed\n",
    "        # got it basically, this gives the best way to combine information accumulated from the attention heads. instead of 6 times 64 we get the best factored and weighted 384\n",
    "        # Without that projection, the heads’ outputs would just sit side by side, which is less expressive because the model can’t mix them\n",
    "        # It’s like having six experts each give their opinion and then using a smart weighted summary to make a final decision.\n",
    "        # okay after projection i have a 64 x 256 x 384 where 64 is batch, 256 is block size and 384 as sequence , what happens in dropout. does some of the values in 384 become zero randomly . i have 20 % dropout\n",
    "        # After the projection, your tensor has shape (B, T, C) → (64, 256, 384). Applying dropout with 20% does the following:\n",
    "        # For each element in that 384-dimensional vector at each (batch, time) position, there’s a 20% chance it will be set to zero.\n",
    "        # The remaining values are scaled up by 1/(1 - 0.2) = 1.25 during training to keep the expected sum consistent.\n",
    "        return   out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51982b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\" # done individually on tokens\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential( # simple linear followed by non linearity\n",
    "            nn.Linear(n_embd, 4 * n_embd), # the factor of 4 is as per Attention is all you need paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # this is the projection layer going back\n",
    "            nn.Dropout(dropout),    # drop out neurons\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# layer norm - batch norm  - across batch dim . zero mean 1 std dev for every columns\n",
    "# layer norm - normalize rows. a 1d vector is normalized. we dont need buffers like batch norm as we are not going across layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer has multiple blocks of multihead attention folowed by feedforward in sequence\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # communication\n",
    "        self.ffwd = FeedFoward(n_embd) # computation\n",
    "        # x′=σx−μ​⋅γ+β where \n",
    "        # μ: mean of the token’s embedding (over its 384 dims)\n",
    "        # σ: standard deviation of that embedding.\n",
    "        # γ, β: learned parameters (so the model can scale/shift after normalizing).\n",
    "        # Keeps activations centered (mean 0) and scaled (variance 1) → avoids exploding/vanishing value\n",
    "        # For each batch b and time step t, it looks at the vector x[b, t, :] of size 384\n",
    "        # Computes mean and variance over those 384 values\n",
    "        # Normalizes the 384 values using that mean and variance\n",
    "        # This is done independently for all b and t, so 64 × 256 vectors are normalized separately\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))  #  # Residual connnections , transform data, skip connection. residual pathway, . gradients hop directly in backprop, residual pathway initially impact less\n",
    "        # over time blocks kick in. # per token transformation\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734f736",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BigramLanguageModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     82\u001b[39m             idx = torch.cat((idx, idx_next), dim=\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# (B, T+1) concatenate along seq dimension dim=1 → new idx is (B, T+1) = (4, 9)\u001b[39;00m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m idx\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m m = \u001b[43mBigramLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m m = m.to(device)\n\u001b[32m     87\u001b[39m logits, loss = m(xb, yb) \u001b[38;5;66;03m# a forward pass with 4 x 8 matrix\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: BigramLanguageModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Bigram model\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # a lookup table 65 number of embeddings, and 65 is embedding dim\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # identity of token a lookup table 65 number of embeddings, and 65 is embedding dim\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # position embedding, for each position in block size a embedding, 0 to block_size-1\n",
    "        # self.sa_head = Head(n_embd) # self attention module\n",
    "        # self.sa_heads = MultiHeadAttention(4, n_embd // 4) # i.e 4 heads of 8-dimensional self-attention. lowers loss from 2.4 to 2.28\n",
    "        # self.ffwd = FeedForward(n_embed)\n",
    "        # self.blocks = nn.Sequential(\n",
    "        #     Block(n_embd, n_head=4),\n",
    "        #     Block(n_embd, n_head=4),\n",
    "        #     Block(n_embd, n_head=4),\n",
    "\n",
    "        # )\n",
    "        # set up number of Blocks based on hyperparameter n_layer \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # takes n_embed as input and vocab_size as output , a simple Fully Connected Neural Network\n",
    "        # print(\"before\", self.token_embedding_table)\n",
    "        '''we have 65 possible tokens (chars in dataset)\n",
    "        Each token is mapped to a vector of size 65.\n",
    "        E = [\n",
    "            [e11, e12, ..., e165],   # embedding vector for token 0\n",
    "            [e21, e22, ..., e265],   # embedding vector for token 1\n",
    "            ...\n",
    "            [e651, ..., e6565]       # embedding vector for token 64\n",
    "            ]\n",
    "\n",
    "        '''\n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # B ,T , C embedding for each batch x time x c\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # arange gives 0 to T-1 # T x C\n",
    "        x = tok_emb + pos_emb # B, T, C\n",
    "        # one block of multihead attention + ffnn\n",
    "        # x = self.sa_heads(x) # apply multi head attention \n",
    "        # x = self.ffwd(x) # B, T, C\n",
    "        # blocks \n",
    "        x = self.blocks(x) # B, T, C\n",
    "        logits = self.lm_head(x) # B x T x vocab _size obtaining the logits from token embedding - we may have 124 or even 256 as embedding space - we use FFNN to get back\n",
    "        # networks get too deep. how to optimize them. \n",
    "    \n",
    "        # a vocab size logits \n",
    "        # the vocab size is 65- all characters of shakesphere. we create a random embedding of 65 x 65 . \n",
    "        # where 65 is vocab size and 65 is the embedding space. They are learnable weights\n",
    "        # During training, they get adjusted so that tokens that appear in similar contexts have similar embeddings.\n",
    "        # The order of tokens will matter downstream when predicting the next token, because the model sees xb[b, :t] as “context”.\n",
    "        # But the embedding of each token by itself does not encode position — it’s just a vector representing that token.\n",
    "        # print(logits)\n",
    "        '''\n",
    "        We have the input as 4 x 8 of the forward encode - and it gives the embedding for each\n",
    "        ''' \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # picks out the row from table B,T,C 4 x 8 x 65\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C ) # we stack the tokens neatly one below another without messing up the order. so we have 32 rows each of 65 numbers as per embedding\n",
    "            targets = targets.view (B*T) # 4 x 8 gives (32,)\n",
    "            loss = F.cross_entropy(logits, targets) # torch wants B x T X C. \n",
    "            # For each row of logits, Treat it as the unnormalized scores for all 65 tokens. Apply softmax internally → get probabilities over 65 classes\n",
    "            # for embeddings of other size 128 dimension etc, we have a linear neural network that projects it back to logits\n",
    "            # self.fc = nn.Linear(embedding_dim, vocab_size)  # 124 → 65\n",
    "            # logits = self.fc(embedding_table[idx])          # shape: (B, T, 65)\n",
    "            # omg that Problem: You cannot feed 124-dimensional embeddings directly to cross-entropy, because cross_entropy expects logits over vocab size (65).\n",
    "            # Solution: add a linear projection: totally solved all my confusion. karpathy basically left this critical step, or he had another video which i \n",
    "            # didnt watch. so basically the embedding is like a input for neural network with its own layers . can be multiple and that outputs the logits and we do \n",
    "            # softmax to get max probability of token and compare that to target. \n",
    "            # solved. all clear now only tell me in this case the input is independently used to generate output token by token \n",
    "            # 24, 43, 58,  5, 57,  1, 46, 43 - are we brute forcing it. like for ex  we use embedding of 58 only to generate the output and not 24,43,58\n",
    "            # The model is very simple: it only looks at the current token to predict the next token\n",
    "            # Input 24 → embedding → predicts next token 43\n",
    "            # Input 43 → embedding → predicts next token 58\n",
    "            # Input 58 → embedding → predicts next token 5\n",
    "            # … and so on\n",
    "            # ✅ So yes, each step is independent in terms of what embedding is used: only the current token’s embedding is fed into the model to predict the next token.\n",
    "                        \n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the latest block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx) # goes to forward function\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C) # pick out only the last element of a B x T  x C logit a  (B, C) = (4, 65) tensor if input is 4 x 8 x 65\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C) # logits to softmax. dim=-1 → apply softmax along the last dimension (here, C, vocab dimension)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) # get only one\n",
    "            # torch.multinomial samples from a categorical distribution defined by probs\n",
    "            # we want the model’s prediction to guide the choice: tokens with higher probabilities are more likely to be picked.\n",
    "            # probs = [0.7, 0.1, 0.1, 0.1]  # vocab size = 4\n",
    "            # torch.multinomial(probs, 1)   # 70% chance to pick token 0, 10% chance for 1,2,3\n",
    "            # If you always picked argmax, it would be deterministic. Multinomial allows stochastic sampling, which is important for generating diverse tex\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) concatenate along seq dimension dim=1 → new idx is (B, T+1) = (4, 9)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "m = m.to(device)\n",
    "logits, loss = m(xb, yb) # a forward pass with 4 x 8 matrix\n",
    "# tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
    "#         [44, 53, 56,  1, 58, 46, 39, 58],\n",
    "#         [52, 58,  1, 58, 46, 39, 58,  1],\n",
    "#         [25, 17, 27, 10,  0, 21,  1, 54]])\n",
    "# targets:\n",
    "# torch.Size([4, 8])\n",
    "# tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
    "#         [53, 56,  1, 58, 46, 39, 58,  1],\n",
    "#         [58,  1, 58, 46, 39, 58,  1, 46],\n",
    "#         [17, 27, 10,  0, 21,  1, 54, 39]])\n",
    "print(loss) # we get scores for torch.Size([4, 8, 65])\n",
    "idx = torch.zeros((1,1), dtype=torch.long) # 0 is a newline character and we start \n",
    "idx = torch.tensor([[5]]) # 5 as the starting\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
    "# this is without history\n",
    "# bigram model. this looks only at last character to predict next. we expand to use history to predict next token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e35ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # for smaller network higher LR \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d627ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6cc088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Step 0, Loss: 4.6973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Loss: 4.4980\n",
      "Step 200, Loss: 4.3540\n",
      "Step 300, Loss: 4.3019\n",
      "Step 400, Loss: 4.1383\n",
      "Step 500, Loss: 4.0842\n",
      "Step 600, Loss: 3.9211\n",
      "Step 700, Loss: 3.8928\n",
      "Step 800, Loss: 3.7173\n",
      "Step 900, Loss: 3.6722\n",
      "Step 1000, Loss: 3.5917\n",
      "Step 1100, Loss: 3.4900\n",
      "Step 1200, Loss: 3.4401\n",
      "Step 1300, Loss: 3.3894\n",
      "Step 1400, Loss: 3.3141\n",
      "Step 1500, Loss: 3.2329\n",
      "Step 1600, Loss: 3.1788\n",
      "Step 1700, Loss: 3.1531\n",
      "Step 1800, Loss: 3.0930\n",
      "Step 1900, Loss: 3.0790\n",
      "Step 2000, Loss: 2.9576\n",
      "Step 2100, Loss: 2.9700\n",
      "Step 2200, Loss: 2.8952\n",
      "Step 2300, Loss: 2.8947\n",
      "Step 2400, Loss: 2.8516\n",
      "Step 2500, Loss: 2.8409\n",
      "Step 2600, Loss: 2.8163\n",
      "Step 2700, Loss: 2.6917\n",
      "Step 2800, Loss: 2.7242\n",
      "Step 2900, Loss: 2.6964\n",
      "Step 3000, Loss: 2.6581\n",
      "Step 3100, Loss: 2.6654\n",
      "Step 3200, Loss: 2.6539\n",
      "Step 3300, Loss: 2.6368\n",
      "Step 3400, Loss: 2.6182\n",
      "Step 3500, Loss: 2.5721\n",
      "Step 3600, Loss: 2.5843\n",
      "Step 3700, Loss: 2.5734\n",
      "Step 3800, Loss: 2.5613\n",
      "Step 3900, Loss: 2.5380\n",
      "Step 4000, Loss: 2.6231\n",
      "Step 4100, Loss: 2.5287\n",
      "Step 4200, Loss: 2.5319\n",
      "Step 4300, Loss: 2.5195\n",
      "Step 4400, Loss: 2.5604\n",
      "Step 4500, Loss: 2.5326\n",
      "Step 4600, Loss: 2.4741\n",
      "Step 4700, Loss: 2.5485\n",
      "Step 4800, Loss: 2.4712\n",
      "Step 4900, Loss: 2.5541\n",
      "Step 5000, Loss: 2.5570\n",
      "Step 5100, Loss: 2.4498\n",
      "Step 5200, Loss: 2.4292\n",
      "Step 5300, Loss: 2.5185\n",
      "Step 5400, Loss: 2.5089\n",
      "Step 5500, Loss: 2.4749\n",
      "Step 5600, Loss: 2.4744\n",
      "Step 5700, Loss: 2.4473\n",
      "Step 5800, Loss: 2.5302\n",
      "Step 5900, Loss: 2.4910\n",
      "Step 6000, Loss: 2.4719\n",
      "Step 6100, Loss: 2.4853\n",
      "Step 6200, Loss: 2.4663\n",
      "Step 6300, Loss: 2.5173\n",
      "Step 6400, Loss: 2.5146\n",
      "Step 6500, Loss: 2.5033\n",
      "Step 6600, Loss: 2.4728\n",
      "Step 6700, Loss: 2.4705\n",
      "Step 6800, Loss: 2.4663\n",
      "Step 6900, Loss: 2.4200\n",
      "Step 7000, Loss: 2.5491\n",
      "Step 7100, Loss: 2.4753\n",
      "Step 7200, Loss: 2.4992\n",
      "Step 7300, Loss: 2.4683\n",
      "Step 7400, Loss: 2.4332\n",
      "Step 7500, Loss: 2.4912\n",
      "Step 7600, Loss: 2.4577\n",
      "Step 7700, Loss: 2.4696\n",
      "Step 7800, Loss: 2.4468\n",
      "Step 7900, Loss: 2.4483\n",
      "Step 8000, Loss: 2.4880\n",
      "Step 8100, Loss: 2.4846\n",
      "Step 8200, Loss: 2.4700\n",
      "Step 8300, Loss: 2.4772\n",
      "Step 8400, Loss: 2.4981\n",
      "Step 8500, Loss: 2.4946\n",
      "Step 8600, Loss: 2.4077\n",
      "Step 8700, Loss: 2.4934\n",
      "Step 8800, Loss: 2.4601\n",
      "Step 8900, Loss: 2.3890\n",
      "Step 9000, Loss: 2.4998\n",
      "Step 9100, Loss: 2.4722\n",
      "Step 9200, Loss: 2.4444\n",
      "Step 9300, Loss: 2.4683\n",
      "Step 9400, Loss: 2.4258\n",
      "Step 9500, Loss: 2.4387\n",
      "Step 9600, Loss: 2.4567\n",
      "Step 9700, Loss: 2.4587\n",
      "Step 9800, Loss: 2.4349\n",
      "Step 9900, Loss: 2.4142\n",
      "Training finished in 140.34 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "batch_size = 128  # bigger batch helps GPU utilization\n",
    "steps = 10000\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets. interval is set to 300, so 10 times loss between 0 to 3000 iterations\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch_device('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # zeroing up the gradients in previous step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training finished in {end_time - start_time:.2f} seconds\")\n",
    "# 2 min 34 secs for cuda 4.6973 to 2.4142. 2 min 20.3 secs in CPU 4.6973 to 2.4142\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c3836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I yome k ee mprop ves.\n",
      "med belenk s w me ave hirmin, baveard.\n",
      "bellfushealmiere ENus\n",
      "BAy he,\n",
      "NARWhan \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "#initially the output is \n",
    "# 'XAQHukRuaRJKXAYtXzfJ:HEPiu--sDioi;ILCo3pHNTmDwJsfheKRxZCFs\n",
    "# lZJ XQc?:s:HEzEnXalEPklcPU cL'DpdLCafBheH\n",
    "\n",
    "# after 10000 steps it is \n",
    "# I yome k ee mprop ves.\n",
    "# med belenk s w me ave hirmin, baveard.\n",
    "# bellfushealmiere ENus\n",
    "# BAy he,\n",
    "# NARWhan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# m = m.to(device)\n",
    "# for steps in range(10000):\n",
    "#     # sample a batch\n",
    "#     xb, yb = get_batch('train')\n",
    "#     xb = xb.to(device)\n",
    "#     yb = yb.to(device)\n",
    "#     # training loop\n",
    "#     logits, loss = m(xb, yb)\n",
    "#     optimizer.zero_grad(set_to_none=True) # zeroing up the gradients in previous step\n",
    "#     loss.backward()\n",
    "#     optimizer.step() # update parameters\n",
    "#     print(loss.item())\n",
    "# # loss goes from for 3.32 to 3.27 in 100 iterations\n",
    "# # 10,000 steps need 1 min 7.5 seconds in CPU. with cuda it is 1 min 11.2 secs for 10,000 from start. loss went from 4.67 to 2.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72741e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokens are not taking to each other. now they talk to each other and figure out what is in context and predict better\n",
    "# estimate loss in multiple batches - trainand validation loss. torch.no_grad() in estimate loss\n",
    "# token in 5th location should talk only to 1,2,3,4 and not 6,7,8\n",
    "# 5th token - 1,2,3,4 averaged out info like a feature. every t tokens calculate avg of vectors\n",
    "\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B): # for each  batch\n",
    "    for t in range(T): # for each block from first to cumulative of last \n",
    "        xprev = x[b,:t+1] # (t,C) always a t+1 x c where t ranges from 0 to some number \n",
    "        xbow[b,t] = torch.mean(xprev, 0) # torch.mean(xprev, 0) computes the mean along the first dimension (rows). It sums up each column of xprev (element-wise across the 4 rows).\n",
    "        #Then divides by the number of rows (t+1 = 4) → gives average per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319ca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(xbow[0])\n",
    "# tensor([[ 0.1808, -0.0700],\n",
    "#         [-0.3596, -0.9152],\n",
    "#         [ 0.6258,  0.0255],\n",
    "#         [ 0.9545,  0.0643],\n",
    "#         [ 0.3612,  1.1679],\n",
    "#         [-1.3499, -0.5102],\n",
    "#         [ 0.2360, -0.2398],\n",
    "#         [-0.9211,  1.5433]])\n",
    "# tensor([[ 0.1808, -0.0700],\n",
    "#         [-0.0894, -0.4926],\n",
    "#         [ 0.1490, -0.3199],\n",
    "#         [ 0.3504, -0.2238],\n",
    "#         [ 0.3525,  0.0545],\n",
    "#         [ 0.0688, -0.0396],\n",
    "#         [ 0.0927, -0.0682],\n",
    "#         [-0.0341,  0.1332]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bcc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "# torch.ones give 3 x 3 of 1 s\n",
    "# troch.tril gives a lower quadrant 1s and upper zeros\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "# this gives the avg of each row in place of 1s, first row has one 1 so it is 1, second row has two 1s so 0.5 , 0.5 each row adds up to 1. so in this way our matrix \n",
    "# multiplication gives avg till that number , basically what we did using for loop above\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9837db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T)) # using the above concept we get T x T - each row\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C). Torch auto adds B times T x T to wei \n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382456c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3: use Softmax\n",
    "# weighted aggregation of lower triangle. the -inf mask is to ensure future tokens are not communicating with past when we do weighted average\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# wei =\n",
    "# [[1.0000, 0.0000, 0.0000],\n",
    "#  [0.5000, 0.5000, 0.0000],\n",
    "# #  [0.3333, 0.3333, 0.3333]]\n",
    "# the end result is averaged of previous context till current token\n",
    "\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d83206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4.3612e-01,  2.6055e-01, -8.7791e-02,  2.4081e-01, -1.2670e-01,\n",
      "          -6.3213e-01,  2.5118e-01, -3.7007e-01,  1.5550e-01,  7.7529e-01,\n",
      "           3.9655e-01,  4.1202e-01, -9.7145e-02,  2.7214e-01, -9.1076e-01,\n",
      "          -5.7588e-01],\n",
      "         [ 4.0732e-01,  1.7379e-01,  1.4678e-01,  1.9324e-01, -1.1125e-01,\n",
      "          -2.2321e-01,  3.1712e-01, -4.8082e-01,  3.4286e-01,  7.2147e-01,\n",
      "           1.8975e-01,  2.7924e-01, -6.0415e-02,  3.9078e-01, -6.3641e-01,\n",
      "          -3.1325e-01],\n",
      "         [ 1.1724e-01,  7.4618e-02,  3.1544e-01,  3.7297e-01, -8.5293e-02,\n",
      "           2.0070e-01,  7.6096e-02, -7.3248e-01,  1.8252e-01,  6.1751e-01,\n",
      "          -3.5668e-01, -1.9321e-01,  1.6650e-01,  5.6478e-01, -2.0256e-01,\n",
      "           8.7309e-03],\n",
      "         [-6.7887e-04,  5.5931e-02,  2.1709e-01,  3.9189e-01,  8.5315e-02,\n",
      "           2.3985e-01,  1.5639e-02, -6.5458e-01,  2.7192e-02,  6.7284e-01,\n",
      "          -3.4941e-01, -1.6289e-01,  2.4242e-01,  5.4952e-01, -1.9463e-02,\n",
      "           2.3055e-01],\n",
      "         [ 3.9551e-02,  1.8454e-01,  2.3456e-01,  2.1410e-01,  1.9884e-01,\n",
      "           2.0197e-01, -1.9296e-01, -4.5366e-01,  2.2954e-03,  4.3683e-01,\n",
      "          -1.9791e-01,  2.4628e-02,  1.9574e-01,  3.7228e-01,  1.8132e-01,\n",
      "           2.4782e-01],\n",
      "         [ 1.8374e-03,  2.4106e-01,  8.1689e-02,  3.3768e-01,  1.8329e-01,\n",
      "           2.1390e-01, -1.8368e-01, -2.1699e-01, -4.2546e-02,  2.7412e-01,\n",
      "          -1.6695e-01,  1.8925e-01,  2.1034e-02,  2.2159e-01,  1.9084e-01,\n",
      "           4.2570e-01],\n",
      "         [ 6.4491e-02,  9.7449e-02,  1.4628e-01,  3.8168e-01,  1.9961e-01,\n",
      "           2.0677e-01, -1.3860e-01, -3.1995e-01, -1.8846e-02,  3.1081e-01,\n",
      "          -3.0306e-01, -4.6852e-04, -4.6017e-03,  3.5066e-01,  2.4869e-01,\n",
      "           3.0932e-01],\n",
      "         [ 4.6667e-02,  2.1289e-02,  1.0332e-01,  4.2363e-01,  1.2910e-01,\n",
      "           1.2664e-01, -1.2275e-01, -2.4262e-01, -5.3734e-02,  2.7534e-01,\n",
      "          -3.2793e-01, -2.5759e-02, -1.9098e-01,  2.8218e-01,  2.1850e-01,\n",
      "           2.3076e-01]],\n",
      "\n",
      "        [[-4.1525e-01, -9.2359e-01, -1.6894e-01,  1.2665e-01, -4.4220e-01,\n",
      "           1.6496e-01,  3.9686e-01, -5.1704e-01,  4.8568e-01,  2.0046e-01,\n",
      "           7.0394e-01,  6.0570e-01,  7.4691e-01,  2.1454e-02,  4.7028e-01,\n",
      "          -4.9938e-01],\n",
      "         [-1.0962e+00, -1.6702e-01,  1.9083e-01,  3.5117e-01,  4.5660e-01,\n",
      "           1.1785e-01, -2.4881e-01,  1.6603e-01,  9.5800e-01, -2.8005e-01,\n",
      "          -9.6153e-02,  8.0165e-02,  5.7073e-01,  8.2434e-02,  1.3595e-01,\n",
      "          -2.8916e-01],\n",
      "         [-5.9364e-01,  1.5545e-01,  8.0885e-02,  3.3553e-01,  3.0896e-01,\n",
      "           1.3169e-01, -1.5667e-01,  1.6075e-01,  4.4864e-01, -2.8861e-01,\n",
      "           7.8903e-02,  1.1141e-01,  2.2358e-01,  1.2540e-01,  2.2730e-01,\n",
      "           4.2177e-02],\n",
      "         [-2.7656e-01,  9.0914e-02,  1.0275e-01,  1.5767e-01,  3.6458e-01,\n",
      "          -8.4025e-02,  6.3701e-02,  6.9697e-02,  4.6829e-01, -5.8720e-02,\n",
      "           3.5867e-01,  1.9022e-01,  2.0814e-02, -1.3769e-02,  2.5811e-01,\n",
      "           6.3828e-02],\n",
      "         [-2.1715e-01,  2.6302e-01,  9.8513e-02,  3.4699e-01,  7.4378e-01,\n",
      "           1.2104e-01, -1.6758e-01,  4.1912e-03,  2.3724e-01,  2.5176e-03,\n",
      "           1.5647e-01,  3.1379e-01,  3.4435e-04,  7.4562e-02,  2.0193e-01,\n",
      "           3.0778e-01],\n",
      "         [-5.2510e-02,  2.9346e-01,  9.4181e-03,  4.3445e-01,  7.8828e-01,\n",
      "           1.6125e-01, -9.7351e-02, -4.4145e-02,  1.7487e-01,  8.4167e-02,\n",
      "           1.0835e-02,  2.7193e-01, -1.3099e-01,  5.5987e-02,  6.1814e-02,\n",
      "           4.5750e-01],\n",
      "         [-9.7786e-02, -1.4327e-01, -4.1168e-02,  2.1460e-01,  7.7104e-02,\n",
      "           8.7253e-02,  9.2232e-02, -6.4225e-03,  4.6024e-01, -1.0882e-02,\n",
      "           1.2811e-01,  4.0027e-01,  4.8243e-02,  3.6753e-02,  4.2581e-01,\n",
      "           2.0881e-02],\n",
      "         [-1.6670e-01,  4.2957e-02, -5.5631e-02,  2.2052e-01,  2.0107e-01,\n",
      "          -1.2060e-01, -1.9272e-02,  1.0580e-01,  3.3337e-01,  2.1149e-02,\n",
      "           1.1166e-01,  2.6134e-01,  5.5466e-02,  5.2640e-02,  3.5153e-01,\n",
      "           2.9074e-02]],\n",
      "\n",
      "        [[ 8.8325e-01, -1.4305e-01, -4.1702e-01, -6.8735e-01, -5.5925e-01,\n",
      "          -7.4114e-01,  2.3625e-01,  4.6523e-04,  4.7101e-01, -9.9773e-01,\n",
      "           6.6452e-01, -6.1185e-02,  5.6705e-01,  2.8596e-01,  1.2417e+00,\n",
      "          -1.4758e-01],\n",
      "         [ 1.0284e+00, -2.3606e-01, -2.7481e-01, -1.1098e-01, -3.9399e-01,\n",
      "          -3.2319e-01,  3.7200e-01,  1.7470e-01,  4.7838e-02, -6.2399e-01,\n",
      "           7.2471e-02, -5.1513e-01,  2.6857e-01,  2.0737e-01,  3.1546e-02,\n",
      "           2.2829e-01],\n",
      "         [ 4.4903e-01,  9.5525e-02,  9.4616e-02,  2.4035e-04, -2.6546e-01,\n",
      "           2.1200e-01,  4.2349e-01,  1.2570e-01,  8.1618e-02, -1.6869e-01,\n",
      "           3.4651e-02, -8.9099e-02,  2.3805e-01, -6.4595e-04, -2.9041e-01,\n",
      "           4.2524e-01],\n",
      "         [ 4.5762e-01,  1.1920e-02,  1.9665e-02, -1.6865e-02, -3.2851e-01,\n",
      "           4.2507e-02,  3.6669e-01,  6.3780e-02, -7.1619e-02, -1.2905e-01,\n",
      "           3.5631e-02, -1.7677e-01,  5.2160e-02,  1.0153e-01, -2.5517e-01,\n",
      "           3.1229e-01],\n",
      "         [ 4.0427e-01,  1.3232e-01, -1.3418e-01, -8.4723e-02, -1.6324e-01,\n",
      "           1.4493e-01,  3.9481e-01,  1.0113e-01, -3.7096e-02, -2.0399e-02,\n",
      "           3.7682e-01,  1.8757e-01,  2.8096e-01, -4.1202e-02, -1.6914e-01,\n",
      "           1.9391e-01],\n",
      "         [ 4.1321e-01,  2.0071e-01, -2.6329e-01, -2.8165e-01, -2.5864e-01,\n",
      "          -5.8325e-03,  4.3485e-01,  2.2876e-01,  1.7264e-01, -1.6423e-01,\n",
      "           6.4587e-01,  3.2091e-01,  4.4931e-01, -1.2791e-01,  8.3143e-03,\n",
      "           1.9721e-01],\n",
      "         [ 3.1533e-01,  2.0827e-01, -2.3948e-01, -3.3890e-01, -2.5652e-01,\n",
      "          -1.2612e-01,  3.9416e-01,  3.1645e-01,  1.1679e-01, -4.4326e-02,\n",
      "           4.3997e-01,  2.8847e-02,  2.4346e-01, -3.6235e-02, -1.7125e-01,\n",
      "           2.0649e-01],\n",
      "         [ 2.6330e-01,  1.9618e-01, -2.2626e-01, -3.0293e-01, -2.7152e-01,\n",
      "          -7.4461e-02,  2.5997e-01,  2.4810e-01, -1.8591e-02,  1.5109e-01,\n",
      "           3.8788e-01,  9.7223e-02,  1.5030e-01, -3.4179e-02, -9.8210e-02,\n",
      "           8.2414e-02]],\n",
      "\n",
      "        [[ 2.4564e-01,  7.0765e-02, -7.4646e-01,  9.5432e-01, -1.3462e+00,\n",
      "          -4.4303e-01, -1.1293e+00,  1.5061e+00,  7.7681e-03, -3.9513e-01,\n",
      "          -4.6563e-01, -1.4424e-01, -2.7552e-01, -9.1175e-01,  1.4213e-01,\n",
      "           2.3918e-01],\n",
      "         [ 3.4722e-02, -2.8485e-01, -3.7238e-01,  6.1569e-01, -1.0397e+00,\n",
      "           1.8520e-01, -6.0070e-01,  8.2094e-01, -1.4181e-01, -4.5792e-01,\n",
      "          -6.2117e-01, -6.7447e-02, -1.1172e-01, -5.1634e-01, -2.2411e-01,\n",
      "           1.2940e-01],\n",
      "         [ 5.9373e-02, -9.1837e-02, -3.1003e-01,  2.2383e-01, -4.5216e-01,\n",
      "           1.8695e-01, -3.4210e-01,  4.9836e-01,  6.0406e-02, -2.6181e-01,\n",
      "          -1.7330e-01,  1.6924e-01, -5.1840e-02, -3.5500e-01, -2.4532e-01,\n",
      "           4.5924e-02],\n",
      "         [ 1.3999e-01, -9.9524e-02, -3.8394e-01,  3.3268e-01, -3.7055e-01,\n",
      "          -1.5999e-01, -2.9269e-01,  5.8163e-01, -2.5358e-01, -1.8675e-01,\n",
      "          -8.0121e-02, -1.5129e-01,  1.5024e-01, -3.3520e-01, -1.9708e-01,\n",
      "          -4.9730e-02],\n",
      "         [-7.1608e-02, -1.1586e-01, -1.6663e-01,  2.4901e-01, -3.2806e-01,\n",
      "           1.6368e-02, -8.6562e-02,  3.8554e-01, -4.7552e-02, -4.2003e-02,\n",
      "          -2.4668e-01, -1.4504e-01,  1.4660e-01, -2.0713e-01, -2.1203e-01,\n",
      "           6.1246e-02],\n",
      "         [ 6.9283e-02, -1.2913e-01, -1.2693e-01,  1.9409e-01, -2.0180e-01,\n",
      "          -5.6371e-03, -5.4606e-02,  3.4709e-01,  7.0594e-03,  6.0823e-02,\n",
      "          -2.4103e-02, -4.9395e-02,  3.2744e-01, -2.1580e-01, -1.4675e-01,\n",
      "           8.7418e-02],\n",
      "         [ 1.6772e-01, -5.7994e-02, -1.1667e-01,  1.6422e-01, -2.7808e-01,\n",
      "           2.5387e-02, -4.9559e-03,  3.5571e-01, -7.2213e-02,  9.7666e-02,\n",
      "           2.8517e-02,  7.3498e-02,  2.1934e-01, -1.1590e-01, -2.0719e-01,\n",
      "           4.1786e-02],\n",
      "         [ 1.1024e-01, -5.0688e-02, -1.4461e-01,  2.6380e-01, -4.1601e-02,\n",
      "          -2.4712e-01,  3.6328e-02,  2.7469e-01, -1.1793e-01,  2.8825e-01,\n",
      "           6.6680e-02, -1.0127e-01,  3.5488e-01, -8.2468e-02, -3.4685e-02,\n",
      "           2.3928e-02]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Self attention\n",
    "# version 4: different importance instead of uniform averaging out\n",
    "# Self attentin. every single token emits a query, key\n",
    "# query - what am i looking for\n",
    "# key - what do i contain\n",
    "# dot product query with all keys . this becomes wei. if key and query are aligned then we get higher\n",
    "\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Single Self Attention Head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # These take in a C and output head_size which is 16\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # B x T x 16\n",
    "q = query(x) # B x T x 16\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5#. k.transpose(−2,−1): swap the last two dims → (B,16,T) \n",
    "# Torch batch matrix multiplication for each Batch we have a T x 16 and we multiply it by a 16 x T which results in a T x T \n",
    "# B , T, 16 x B x 16 X T --> B , T , T \n",
    "# the division by sqrt head_size is done as if not the softmax sharpens or gets a higher probability for the highest value much higher than others\n",
    "# so the aggregation of information is focussed on only on the biggest value instead of spread out\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x)\n",
    "# out = wei @ x # we dont aggregate tokens\n",
    "out = wei @ v # we agregate values according to wei\n",
    "print(out) # 4 x 8 x 16 dim . heres what i am interested in, heres what i have, heres what i will communicate if you find me interesting (V). V is getting aggregated here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36115e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4.3612e-01,  2.6055e-01, -8.7791e-02,  2.4081e-01, -1.2670e-01,\n",
      "          -6.3213e-01,  2.5118e-01, -3.7007e-01,  1.5550e-01,  7.7529e-01,\n",
      "           3.9655e-01,  4.1202e-01, -9.7145e-02,  2.7214e-01, -9.1076e-01,\n",
      "          -5.7588e-01],\n",
      "         [ 3.0401e-01, -1.3744e-01,  9.8834e-01,  2.2601e-02, -5.5825e-02,\n",
      "           1.2438e+00,  5.5367e-01, -8.7818e-01,  1.0150e+00,  5.2840e-01,\n",
      "          -5.5217e-01, -1.9714e-01,  7.1362e-02,  8.1641e-01,  3.4788e-01,\n",
      "           6.2898e-01],\n",
      "         [-5.0317e-01,  1.6584e-02,  2.3979e-01,  8.8837e-01, -5.5558e-02,\n",
      "           3.7241e-01, -6.1939e-01, -1.0989e+00, -5.6760e-01,  4.8231e-01,\n",
      "          -1.2197e+00, -1.0316e+00,  6.2335e-01,  7.3385e-01,  2.6198e-01,\n",
      "           2.3525e-01],\n",
      "         [-6.1592e-01,  5.6083e-02, -5.8237e-01,  5.6702e-01,  9.7417e-01,\n",
      "          -2.4427e-02, -4.0814e-01, -1.2484e-01, -1.0444e+00,  1.0246e+00,\n",
      "          -1.0066e-01,  1.2252e-01,  6.1935e-01,  3.3588e-01,  6.4877e-01,\n",
      "           1.1093e+00],\n",
      "         [ 4.2123e-01,  7.1757e-01,  5.8130e-01, -5.6039e-01,  3.2861e-01,\n",
      "           1.1087e-01, -9.1255e-01,  1.7019e-01,  2.6425e-01, -6.5928e-01,\n",
      "           3.2526e-01,  6.7478e-01, -1.2798e-01, -2.7957e-01,  7.3743e-01,\n",
      "          -3.1286e-02],\n",
      "         [ 2.3433e-01,  4.8029e-01, -2.8840e-01,  8.3010e-01, -3.7797e-01,\n",
      "           3.7675e-01,  3.1789e-01,  7.0689e-01,  4.3163e-01, -5.7246e-01,\n",
      "           9.2327e-04,  9.8536e-01, -1.1173e+00, -3.8974e-01, -2.7821e-01,\n",
      "           8.3169e-01],\n",
      "         [ 7.7928e-01, -8.4690e-01,  2.9499e-01,  4.1403e-01,  8.2228e-01,\n",
      "          -2.3532e-01,  2.5831e-02, -2.1575e-01,  3.0207e-02,  1.7596e-01,\n",
      "          -6.7366e-01, -6.6509e-01, -6.4248e-01,  8.2270e-01,  1.0977e+00,\n",
      "          -4.4796e-01],\n",
      "         [-8.0078e-01, -3.5222e-02, -1.4922e-01,  7.7803e-01, -5.6809e-01,\n",
      "           1.9518e-01, -3.9321e-01,  1.4361e-02, -5.5234e-01,  8.7435e-02,\n",
      "          -7.1040e-01, -2.0769e-01, -7.9711e-01, -3.1051e-01,  1.1401e-02,\n",
      "           4.8245e-01]],\n",
      "\n",
      "        [[-4.1525e-01, -9.2359e-01, -1.6894e-01,  1.2665e-01, -4.4220e-01,\n",
      "           1.6496e-01,  3.9686e-01, -5.1704e-01,  4.8568e-01,  2.0046e-01,\n",
      "           7.0394e-01,  6.0570e-01,  7.4691e-01,  2.1454e-02,  4.7028e-01,\n",
      "          -4.9938e-01],\n",
      "         [-1.4658e+00,  2.4355e-01,  3.8607e-01,  4.7302e-01,  9.4436e-01,\n",
      "           9.2279e-02, -5.9920e-01,  5.3671e-01,  1.2143e+00, -5.4082e-01,\n",
      "          -5.3034e-01, -2.0503e-01,  4.7512e-01,  1.1553e-01, -4.5483e-02,\n",
      "          -1.7508e-01],\n",
      "         [ 2.9749e-01,  1.1150e+00, -4.5553e-02,  3.7386e-01,  2.7424e-01,\n",
      "           1.4701e-01, -1.6254e-01,  3.7104e-01, -5.2527e-01, -4.6369e-01,\n",
      "           2.0637e-01,  9.8015e-03, -6.0203e-01,  2.4032e-01,  3.2084e-01,\n",
      "           8.4383e-01],\n",
      "         [ 4.0765e-01,  1.8394e-01,  2.3017e-01, -2.2118e-01,  7.0715e-01,\n",
      "          -6.2037e-01,  4.5070e-01,  2.2885e-02,  5.5099e-01,  3.7691e-01,\n",
      "           8.6499e-01,  2.5224e-01, -5.7905e-01, -3.3039e-01,  2.6264e-01,\n",
      "           2.2768e-01],\n",
      "         [-1.0829e-01,  1.6305e-01,  3.1542e-02,  7.4672e-01,  1.5345e+00,\n",
      "           6.4253e-01, -5.6235e-01, -4.6191e-01, -1.8130e-01,  4.1351e-01,\n",
      "          -1.4977e-01,  8.5135e-01,  2.4421e-01,  2.2881e-01,  1.1969e-01,\n",
      "           6.3236e-01],\n",
      "         [ 1.9920e-01,  1.6484e-01, -4.1476e-01,  1.0734e+00,  1.1903e+00,\n",
      "           4.8132e-01,  1.1913e-01, -2.5240e-01,  3.9810e-01,  4.4967e-01,\n",
      "          -1.2435e+00, -1.0140e-01, -4.5217e-01, -4.1940e-02, -9.3126e-01,\n",
      "           9.7973e-01],\n",
      "         [ 8.5294e-01, -2.2776e-01, -1.8330e-01, -2.3977e-01, -1.3941e+00,\n",
      "           6.0772e-02,  1.5378e-01,  5.5440e-01,  4.9284e-01, -4.2642e-01,\n",
      "          -1.5337e-01,  9.8704e-01, -5.3990e-01,  1.6204e-01,  1.7925e+00,\n",
      "          -2.0134e-01],\n",
      "         [-8.4818e-01,  5.9130e-01, -3.5918e-01,  9.0794e-02,  1.9178e-01,\n",
      "          -1.6500e+00, -4.7914e-01,  6.1540e-01, -4.4871e-01,  3.5271e-01,\n",
      "           3.7443e-01, -5.6061e-01,  6.7003e-01,  1.5799e-01,  5.7784e-02,\n",
      "          -5.2942e-01]],\n",
      "\n",
      "        [[ 8.8325e-01, -1.4305e-01, -4.1702e-01, -6.8735e-01, -5.5925e-01,\n",
      "          -7.4114e-01,  2.3625e-01,  4.6523e-04,  4.7101e-01, -9.9773e-01,\n",
      "           6.6452e-01, -6.1185e-02,  5.6705e-01,  2.8596e-01,  1.2417e+00,\n",
      "          -1.4758e-01],\n",
      "         [ 1.2099e+00, -3.5247e-01, -9.6827e-02,  6.1037e-01, -1.8717e-01,\n",
      "           1.9988e-01,  5.4191e-01,  3.9275e-01, -4.8176e-01, -1.5624e-01,\n",
      "          -6.6849e-01, -1.0832e+00, -1.0498e-01,  1.0901e-01, -1.4830e+00,\n",
      "           6.9870e-01],\n",
      "         [-3.7597e-01,  5.6819e-01,  6.0973e-01,  1.3040e-01, -9.1652e-02,\n",
      "           9.4837e-01,  4.8988e-01,  4.8362e-02,  1.4909e-01,  4.5689e-01,\n",
      "           8.8861e-03,  5.3381e-01,  2.0889e-01, -2.9077e-01, -6.8862e-01,\n",
      "           6.8585e-01],\n",
      "         [ 2.3580e-01, -1.9346e-01, -1.2799e-01, -2.8806e-02, -5.3389e-01,\n",
      "          -4.2476e-01,  1.5249e-01, -2.1655e-01, -7.1077e-01,  2.4979e-01,\n",
      "           1.2249e-02, -3.6324e-01, -7.5156e-01,  4.3783e-01, -2.7272e-01,\n",
      "          -6.9205e-02],\n",
      "         [ 2.5683e-01,  5.5467e-01, -6.4230e-01, -4.5970e-01,  3.3412e-01,\n",
      "           4.0406e-01,  4.7805e-01,  2.1724e-01,  2.9440e-01,  1.3225e-01,\n",
      "           1.6099e+00,  1.4635e+00,  1.2120e+00, -5.1154e-01,  4.5303e-01,\n",
      "          -2.2367e-01],\n",
      "         [ 1.1187e-01,  5.4396e-01, -2.7875e-01, -5.2129e-01, -7.4519e-01,\n",
      "          -1.2930e-01,  7.4055e-01,  9.1138e-01,  6.6169e-01, -1.8109e-01,\n",
      "           9.2454e-01,  3.8224e-01,  4.8736e-01, -5.7115e-01, -7.2715e-01,\n",
      "           8.5318e-01],\n",
      "         [-5.3731e-01,  5.4584e-01, -5.4314e-01, -1.3660e+00,  1.9716e-01,\n",
      "          -9.2269e-01, -7.1355e-02,  3.9060e-01,  1.2279e-01,  5.9508e-01,\n",
      "           5.6662e-01, -2.6746e-01, -1.3463e-01,  4.3100e-01,  6.3542e-01,\n",
      "          -7.1791e-01],\n",
      "         [ 2.2055e-01,  6.0831e-02, -3.1967e-01, -1.2542e-01, -6.1821e-01,\n",
      "           2.9247e-02, -4.9855e-01,  1.8859e-01, -7.1420e-01,  1.1756e+00,\n",
      "           5.5962e-02,  2.4766e-01, -3.7343e-01, -1.4503e-01,  7.6095e-02,\n",
      "          -4.6476e-01]],\n",
      "\n",
      "        [[ 2.4564e-01,  7.0765e-02, -7.4646e-01,  9.5432e-01, -1.3462e+00,\n",
      "          -4.4303e-01, -1.1293e+00,  1.5061e+00,  7.7681e-03, -3.9513e-01,\n",
      "          -4.6563e-01, -1.4424e-01, -2.7552e-01, -9.1175e-01,  1.4213e-01,\n",
      "           2.3918e-01],\n",
      "         [-2.2377e-01, -7.2067e-01,  8.6097e-02,  2.0067e-01, -6.6398e-01,\n",
      "           9.5515e-01,  4.7095e-02, -1.8731e-02, -3.2512e-01, -5.3487e-01,\n",
      "          -8.1180e-01,  2.6668e-02,  8.9035e-02, -3.1733e-02, -6.7296e-01,\n",
      "          -5.1478e-03],\n",
      "         [ 1.4988e-01,  3.0934e-01, -2.9647e-01, -3.3898e-01,  4.3940e-01,\n",
      "           4.0752e-02, -4.3071e-02,  1.3148e-01,  4.2780e-01,  7.4976e-02,\n",
      "           5.9871e-01,  5.3950e-01,  7.5663e-03, -1.8401e-01, -1.9322e-01,\n",
      "          -6.5034e-02],\n",
      "         [ 4.5477e-01,  1.7537e-01, -6.5249e-01,  3.7913e-01,  3.0227e-01,\n",
      "          -1.3229e+00, -8.7994e-02,  7.3525e-01, -8.8419e-01,  2.2400e-01,\n",
      "           6.4567e-01, -8.4188e-01,  7.1109e-01, -2.5944e-01,  3.6252e-02,\n",
      "          -3.5603e-01],\n",
      "         [-7.4249e-01, -6.9391e-02,  5.2441e-01,  8.1052e-02,  6.8518e-02,\n",
      "          -2.3995e-02,  7.9375e-01, -1.9536e-01,  3.1757e-01,  7.5001e-01,\n",
      "          -6.1849e-01, -6.5505e-01,  4.7292e-01,  3.0529e-01, -1.0945e-01,\n",
      "           3.4940e-01],\n",
      "         [ 5.7658e-01, -2.6402e-01,  2.9743e-01, -1.8268e-01,  3.8244e-01,\n",
      "           2.4706e-01,  2.4460e-01, -5.3503e-02,  5.5857e-01,  6.2099e-01,\n",
      "           8.7583e-01,  5.8929e-01,  1.1550e+00, -1.6468e-01,  1.2196e-01,\n",
      "           3.5293e-01],\n",
      "         [ 5.3395e-01,  2.5219e-01,  1.0294e-01, -2.0394e-02, -6.8958e-01,\n",
      "           2.5224e-01,  5.8455e-01,  1.9618e-01, -7.7089e-01,  3.6924e-01,\n",
      "           8.4383e-02,  5.1257e-01, -3.6766e-01,  7.0348e-01, -7.0086e-01,\n",
      "          -2.8821e-01],\n",
      "         [-4.6677e-02, -4.9558e-01, -5.3059e-01,  1.1371e+00,  5.5927e-01,\n",
      "          -5.5360e-01, -6.7379e-01, -2.3737e-01,  8.8243e-02,  6.1701e-01,\n",
      "          -2.4005e-01,  2.5683e-01,  3.4071e-01, -2.6030e-01,  9.4120e-01,\n",
      "           5.0612e-02]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4822, 0.5178, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5354, 0.2838, 0.1808, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1647, 0.0943, 0.1177, 0.6233, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7846, 0.0586, 0.0478, 0.0476, 0.0614, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1013, 0.2149, 0.1094, 0.3307, 0.1485, 0.0952, 0.0000, 0.0000],\n",
      "        [0.0458, 0.0425, 0.2363, 0.1927, 0.1673, 0.1253, 0.1901, 0.0000],\n",
      "        [0.0423, 0.0665, 0.1825, 0.0205, 0.1035, 0.0896, 0.2237, 0.2714]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(wei[0])\n",
    "# attention is a communication mechanism . every node as vector of info, we aggregate info as weighted sum from all nodes pointing to it\n",
    "# attention acts over set of vectors without notion of space, so we positionally encode tokens\n",
    "# element across batch dont talk to each other. we have 4 pool of 8 nodes, who only talk among those 8 within the batch. only to past\n",
    "# sentiment analysis - all token talk to each other. in encoder block we delete wei = wei.masked_fill(tril == 0, float('-inf')) to enable \n",
    "# cross attention - q,k,v from same source x. self attended. in encoded-decoder block. q come from some source and k,v from another source. seperate source\n",
    "# where we pool info\n",
    "# now this contains diff values - information aggregated from tokens in past\n",
    "# tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "#         [0.4822, 0.5178, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "#         [0.5354, 0.2838, 0.1808, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "#         [0.1647, 0.0943, 0.1177, 0.6233, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "#         [0.7846, 0.0586, 0.0478, 0.0476, 0.0614, 0.0000, 0.0000, 0.0000],\n",
    "#         [0.1013, 0.2149, 0.1094, 0.3307, 0.1485, 0.0952, 0.0000, 0.0000],\n",
    "#         [0.0458, 0.0425, 0.2363, 0.1927, 0.1673, 0.1253, 0.1901, 0.0000],\n",
    "#         [0.0423, 0.0665, 0.1825, 0.0205, 0.1035, 0.0896, 0.2237, 0.2714]],"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
