{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "# a sequence of tokens, how they are sequenced, we give start of sequence the model continues\n",
    "# Generatively Pre-trained Transformer\n",
    "# Character level language model\n",
    "# Tiny shakesphere - concatenated work of all shakesphere in text filter\n",
    "# We model how charcters follow if given context\n",
    "# Output is a shakesphere like text\n",
    "# NanoGPT . train GPT with any text 124 million params - 38 hours training time in a single 8xA100 40GB Node\n",
    "#  8xA100 40GB - One machine with 8 NVIDIA A100 GPUs, each with 40â€¯GB of VRAM, connected via NVLink or PCIe for multi-GPU training\n",
    "# using OpenWebText data \n",
    "# OpenwebText has 38GB of text data (40GB using SI units) from 8,013,769 documents\n",
    "# train.py is 300 line training loop\n",
    "# model.py a 300 line GPT model definition\n",
    "# A multilayer perceptron is a fully connected feedforward net with at least three layers (input, hidden and output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d5854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
